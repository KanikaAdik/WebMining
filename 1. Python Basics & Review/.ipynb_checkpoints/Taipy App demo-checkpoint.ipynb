{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce45c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting taipy\n",
      "  Downloading taipy-2.0.0-py3-none-any.whl (8.1 kB)\n",
      "Collecting taipy-gui<2.1,>=2.0\n",
      "  Downloading taipy_gui-2.0.4-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting taipy-rest<2.1,>=2.0\n",
      "  Downloading taipy_rest-2.0.0-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pytz<2022.2,>=2021.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-gui<2.1,>=2.0->taipy) (2021.3)\n",
      "Collecting flask<2.3,>=2.2\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 11.4 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pandas<2.0,>=1.4.4\n",
      "  Downloading pandas-1.5.3-cp39-cp39-macosx_10_9_x86_64.whl (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gevent-websocket<0.11,>=0.10.1\n",
      "  Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
      "Collecting flask-socketio<6.0,>=5.3.0\n",
      "  Downloading Flask_SocketIO-5.3.2-py3-none-any.whl (17 kB)\n",
      "Collecting markdown<4.0,>=3.4.1\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting taipy-config<3.0,>=2.0\n",
      "  Downloading taipy_config-2.1.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting kthread<0.3,>=0.2.3\n",
      "  Downloading kthread-0.2.3-py3-none-any.whl (3.9 kB)\n",
      "Collecting flask-cors<4.0,>=3.0.10\n",
      "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting tzlocal<5.0,>=3.0\n",
      "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting python-dotenv<0.21,>=0.19\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Collecting gevent<22.0,>=21.12.0\n",
      "  Downloading gevent-21.12.0-cp39-cp39-macosx_10_14_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 23.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (4.8.1)\n",
      "Requirement already satisfied: Six in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask-cors<4.0,>=3.0.10->taipy-gui<2.1,>=2.0->taipy) (1.16.0)\n",
      "Collecting python-socketio>=5.0.2\n",
      "  Downloading python_socketio-5.7.2-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zope.interface in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (58.0.4)\n",
      "Requirement already satisfied: zope.event in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (4.5.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (3.6.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp39-cp39-macosx_10_9_x86_64.whl (13 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0,>=1.4.4->taipy-gui<2.1,>=2.0->taipy) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0,>=1.4.4->taipy-gui<2.1,>=2.0->taipy) (2.8.2)\n",
      "Collecting python-engineio>=4.3.0\n",
      "  Downloading python_engineio-4.3.4-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting bidict>=0.21.0\n",
      "  Downloading bidict-0.22.1-py3-none-any.whl (35 kB)\n",
      "Collecting deepdiff<6.3,>=6.2\n",
      "  Downloading deepdiff-6.2.3-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml<0.11,>=0.10 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-config<3.0,>=2.0->taipy-gui<2.1,>=2.0->taipy) (0.10.2)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.5-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (489 kB)\n",
      "\u001b[K     |████████████████████████████████| 489 kB 20.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ordered-set<4.2.0,>=4.0.2\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting passlib<1.8,>=1.7.4\n",
      "  Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
      "\u001b[K     |████████████████████████████████| 525 kB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marshmallow-sqlalchemy<0.29,>=0.25\n",
      "  Downloading marshmallow_sqlalchemy-0.28.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting apispec[yaml]<6.0,>=5.1\n",
      "  Downloading apispec-5.2.2-py3-none-any.whl (29 kB)\n",
      "Collecting flask-migrate<4.0,>=3.1\n",
      "  Downloading Flask_Migrate-3.1.0-py3-none-any.whl (20 kB)\n",
      "Collecting flask-jwt-extended<5.0,>=4.3\n",
      "  Downloading Flask_JWT_Extended-4.4.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting taipy-core<2.1,>=2.0\n",
      "  Downloading taipy_core-2.0.4-py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flask-marshmallow<0.15,>=0.14\n",
      "  Downloading flask_marshmallow-0.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting flask-restful<0.4,>=0.3.9\n",
      "  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "Collecting apispec-webframeworks<0.6,>=0.5.2\n",
      "  Downloading apispec_webframeworks-0.5.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from apispec[yaml]<6.0,>=5.1->taipy-rest<2.1,>=2.0->taipy) (6.0)\n",
      "Requirement already satisfied: PyJWT<3.0,>=2.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask-jwt-extended<5.0,>=4.3->taipy-rest<2.1,>=2.0->taipy) (2.1.0)\n",
      "Collecting marshmallow>=2.0.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic>=0.7\n",
      "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 28.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Flask-SQLAlchemy>=1.0\n",
      "  Downloading Flask_SQLAlchemy-3.0.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.3.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from alembic>=0.7->flask-migrate<4.0,>=3.1->taipy-rest<2.1,>=2.0->taipy) (1.4.22)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aniso8601>=0.82\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from marshmallow>=2.0.0->flask-marshmallow<0.15,>=0.14->taipy-rest<2.1,>=2.0->taipy) (21.0)\n",
      "Collecting packaging>=17.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 7.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting taipy-config<3.0,>=2.0\n",
      "  Downloading taipy_config-2.0.1-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: openpyxl<4.0,>=3.0.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-core<2.1,>=2.0->taipy-rest<2.1,>=2.0->taipy) (3.0.9)\n",
      "Requirement already satisfied: networkx<3.0,>=2.6 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-core<2.1,>=2.0->taipy-rest<2.1,>=2.0->taipy) (2.6.3)\n",
      "Requirement already satisfied: et-xmlfile in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from openpyxl<4.0,>=3.0.3->taipy-core<2.1,>=2.0->taipy-rest<2.1,>=2.0->taipy) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
      "\u001b[K     |████████████████████████████████| 340 kB 43.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: MarkupSafe, Werkzeug, Jinja2, tzdata, python-engineio, packaging, Mako, flask, bidict, apispec, taipy-config, pytz-deprecation-shim, python-socketio, pandas, marshmallow, gevent, Flask-SQLAlchemy, aniso8601, alembic, tzlocal, taipy-core, python-dotenv, passlib, marshmallow-sqlalchemy, markdown, kthread, gevent-websocket, flask-socketio, flask-restful, flask-migrate, flask-marshmallow, flask-jwt-extended, flask-cors, apispec-webframeworks, taipy-rest, taipy-gui, taipy\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.0.2\n",
      "    Uninstalling Werkzeug-2.0.2:\n",
      "      Successfully uninstalled Werkzeug-2.0.2\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.0\n",
      "    Uninstalling packaging-21.0:\n",
      "      Successfully uninstalled packaging-21.0\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 1.1.2\n",
      "    Uninstalling Flask-1.1.2:\n",
      "      Successfully uninstalled Flask-1.1.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\n",
      "  Attempting uninstall: gevent\n",
      "    Found existing installation: gevent 21.8.0\n",
      "    Uninstalling gevent-21.8.0:\n",
      "      Successfully uninstalled gevent-21.8.0\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.3.7\n",
      "    Uninstalling Markdown-3.3.7:\n",
      "      Successfully uninstalled Markdown-3.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "cookiecutter 1.7.2 requires Jinja2<3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
      "cookiecutter 1.7.2 requires MarkupSafe<2.0.0, but you have markupsafe 2.1.2 which is incompatible.\u001b[0m\n",
      "Successfully installed Flask-SQLAlchemy-3.0.3 Jinja2-3.1.2 Mako-1.2.4 MarkupSafe-2.1.2 Werkzeug-2.2.2 alembic-1.9.2 aniso8601-9.0.1 apispec-5.2.2 apispec-webframeworks-0.5.2 bidict-0.22.1 flask-2.2.2 flask-cors-3.0.10 flask-jwt-extended-4.4.4 flask-marshmallow-0.14.0 flask-migrate-3.1.0 flask-restful-0.3.9 flask-socketio-5.3.2 gevent-21.12.0 gevent-websocket-0.10.1 kthread-0.2.3 markdown-3.4.1 marshmallow-3.19.0 marshmallow-sqlalchemy-0.28.1 packaging-23.0 pandas-1.5.3 passlib-1.7.4 python-dotenv-0.20.0 python-engineio-4.3.4 python-socketio-5.7.2 pytz-deprecation-shim-0.1.0.post0 taipy-2.0.0 taipy-config-2.0.1 taipy-core-2.0.4 taipy-gui-2.0.4 taipy-rest-2.0.0 tzdata-2022.7 tzlocal-4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install taipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decfe8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b5636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: scipy>=1.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (1.7.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.21 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.21->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.21->statsmodels) (2021.3)\n",
      "Requirement already satisfied: six in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from patsy>=0.5->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ed8947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1fc9878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "from taipy import Gui\n",
    "Gui(page=\"# Getting started with *Taipy*\").run(dark_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c61cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data(path_to_csv: str):\n",
    "    # pandas.read_csv() returns a pd.DataFrame\n",
    "    dataset = pd.read_csv(path_to_csv)\n",
    "    dataset[\"Date\"] = pd.to_datetime(dataset[\"Date\"])\n",
    "    return dataset\n",
    "\n",
    "# Read the dataframe\n",
    "path_to_csv = \"dataset.csv\"\n",
    "dataset = get_data(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd499be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "from taipy import Gui\n",
    "\n",
    "dataset = get_data(path_to_csv)\n",
    "\n",
    "# Initial value\n",
    "n_week = 10\n",
    "\n",
    "# Definition of the page\n",
    "page = \"\"\"\n",
    "# Getting started with Taipy\n",
    "\n",
    "Week number: *<|{n_week}|>*\n",
    "\n",
    "Interact with this slider to change the week number:\n",
    "<|{n_week}|slider|min=1|max=52|>\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "Display the last three months of data:\n",
    "<|{dataset[9000:]}|chart|type=bar|x=Date|y=Value|height=100%|>\n",
    "\n",
    "<|{dataset}|table|height=400px|width=95%|>\n",
    "\"\"\"\n",
    "\n",
    "# Create a Gui object with our page content\n",
    "Gui(page=page).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b895f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# Select the week based on the slider value\n",
    "dataset_week = dataset[dataset[\"Date\"].dt.isocalendar().week == n_week]\n",
    "\n",
    "page = \"\"\"\n",
    "# Getting started with Taipy\n",
    "\n",
    "Select week: *<|{n_week}|>*\n",
    "\n",
    "<|{n_week}|slider|min=1|max=52|>\n",
    "\n",
    "<|{dataset_week}|chart|type=bar|x=Date|y=Value|height=100%|width=100%|>\n",
    "\"\"\"\n",
    "\n",
    "# on_change is the function that is called when any variable is changed\n",
    "def on_change(state, var_name: str, var_value):\n",
    "    if var_name == \"n_week\":\n",
    "        # Update the dataset when the slider is moved\n",
    "        state.dataset_week = dataset[dataset[\"Date\"].dt.isocalendar().week == var_value]\n",
    "\n",
    "Gui(page=page).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34fadfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from taipy import Config, Scope\n",
    "\n",
    "## Input Data Nodes\n",
    "initial_dataset_cfg = Config.configure_data_node(id=\"initial_dataset\",\n",
    "                                                 storage_type=\"csv\",\n",
    "                                                 path=path_to_csv,\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "\n",
    "# We assume the current day is the 26th of July 2021.\n",
    "# This day can be changed to simulate multiple executions of scenarios on different days\n",
    "day_cfg = Config.configure_data_node(id=\"day\", default_data=dt.datetime(2021, 7, 26))\n",
    "\n",
    "n_predictions_cfg = Config.configure_data_node(id=\"n_predictions\", default_data=40)\n",
    "\n",
    "max_capacity_cfg = Config.configure_data_node(id=\"max_capacity\", default_data=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955eba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from taipy import Config, Scope\n",
    "\n",
    "## Input Data Nodes\n",
    "initial_dataset_cfg = Config.configure_data_node(id=\"initial_dataset\",\n",
    "                                                 storage_type=\"csv\",\n",
    "                                                 path=path_to_csv,\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "\n",
    "# We assume the current day is the 26th of July 2021.\n",
    "# This day can be changed to simulate multiple executions of scenarios on different days\n",
    "day_cfg = Config.configure_data_node(id=\"day\", default_data=dt.datetime(2021, 7, 26))\n",
    "\n",
    "n_predictions_cfg = Config.configure_data_node(id=\"n_predictions\", default_data=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "164098c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remaining Data Nodes\n",
    "cleaned_dataset_cfg = Config.configure_data_node(id=\"cleaned_dataset\",\n",
    "                                             cacheable=True,\n",
    "                                             validity_period=dt.timedelta(days=1),\n",
    "                                             scope=Scope.GLOBAL) \n",
    "\n",
    "predictions_cfg = Config.configure_data_node(id=\"predictions\", scope=Scope.PIPELINE)\n",
    "\n",
    "def clean_data(initial_dataset: pd.DataFrame):\n",
    "    print(\"     Cleaning data\")\n",
    "    # Convert the date column to datetime\n",
    "    initial_dataset[\"Date\"] = pd.to_datetime(initial_dataset[\"Date\"])\n",
    "    cleaned_dataset = initial_dataset.copy()\n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def predict_baseline(cleaned_dataset: pd.DataFrame, n_predictions: int, day: dt.datetime, max_capacity: int):\n",
    "    print(\"     Predicting baseline\")\n",
    "    # Select the train data\n",
    "    train_dataset = cleaned_dataset[cleaned_dataset[\"Date\"] < day]\n",
    "\n",
    "    predictions = train_dataset[\"Value\"][-n_predictions:].reset_index(drop=True)\n",
    "    predictions = predictions.apply(lambda x: min(x, max_capacity))\n",
    "    return predictions\n",
    "\n",
    "clean_data_task_cfg = Config.configure_task(id=\"clean_data\",\n",
    "                                            function=clean_data,\n",
    "                                            input=initial_dataset_cfg,\n",
    "                                            output=cleaned_dataset_cfg)\n",
    "\n",
    "predict_baseline_task_cfg = Config.configure_task(id=\"predict_baseline\",\n",
    "                                                  function=predict_baseline,\n",
    "                                                  input=[cleaned_dataset_cfg, n_predictions_cfg, day_cfg, max_capacity_cfg],\n",
    "                                                  output=predictions_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3785003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Cleaning data\n",
      "[2023-01-31 15:07:15,147][Taipy][INFO] job JOB_clean_data_aef760bf-c987-4980-aafa-55689051d1fb is completed.\n",
      "     Predicting baseline\n",
      "[2023-01-31 15:07:15,169][Taipy][INFO] job JOB_predict_baseline_edaf0a22-5fb1-4397-a119-9677b3121b7b is completed.\n",
      "Predictions of baseline algorithm\n",
      " 0     128\n",
      "1     108\n",
      "2     106\n",
      "3      99\n",
      "4      84\n",
      "5     107\n",
      "6     118\n",
      "7      93\n",
      "8     121\n",
      "9      98\n",
      "10    108\n",
      "11     99\n",
      "12    114\n",
      "13     90\n",
      "14    129\n",
      "15     87\n",
      "16    102\n",
      "17    120\n",
      "18    110\n",
      "19    166\n",
      "20    109\n",
      "21    109\n",
      "22     70\n",
      "23    132\n",
      "24     57\n",
      "25    140\n",
      "26    137\n",
      "27     98\n",
      "28    119\n",
      "29    130\n",
      "30    191\n",
      "31    143\n",
      "32    132\n",
      "33    159\n",
      "34    184\n",
      "35    101\n",
      "36    119\n",
      "37     62\n",
      "38     98\n",
      "39      8\n",
      "Name: Value, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Pipeline\n",
    "baseline_pipeline_cfg = Config.configure_pipeline(id=\"baseline\",\n",
    "                                                  task_configs=[clean_data_task_cfg, predict_baseline_task_cfg])\n",
    "import taipy as tp\n",
    "\n",
    "# Create the pipeline\n",
    "baseline_pipeline = tp.create_pipeline(baseline_pipeline_cfg)\n",
    "# Submit the pipeline (Execution)\n",
    "tp.submit(baseline_pipeline)\n",
    "\n",
    "# Read output data from the pipeline\n",
    "baseline_predictions = baseline_pipeline.predictions.read()\n",
    "print(\"Predictions of baseline algorithm\\n\", baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bc882f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the \"predictions\" dataset\n",
    "predictions_dataset = pd.DataFrame({\"Date\":[dt.datetime(2021, 6, 1)], \"Historical values\":[np.NaN], \"Predicted values\":[np.NaN]})\n",
    "\n",
    "# Add a button and a chart for our predictions\n",
    "pipeline_page = page + \"\"\"\n",
    "Press <|predict|button|on_action=predict|> to predict with default parameters (30 predictions) and June 1st as day.\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c19700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(state):\n",
    "    print(\"'Predict' button clicked\")\n",
    "    pipeline = create_and_submit_pipeline()\n",
    "    update_predictions_dataset(state, pipeline)\n",
    "\n",
    "\n",
    "def create_and_submit_pipeline():\n",
    "    print(\"Execution of pipeline...\")\n",
    "    # Create the pipeline from the pipeline config\n",
    "    pipeline = tp.create_pipeline(baseline_pipeline_cfg)\n",
    "    # Submit the pipeline (Execution)\n",
    "    tp.submit(pipeline)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d35a03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_dataset(pipeline):\n",
    "    print(\"Creating predictions dataset...\")\n",
    "    # Read data from the pipeline\n",
    "    predictions = pipeline.predictions.read()\n",
    "    day = pipeline.day.read()\n",
    "    n_predictions = pipeline.n_predictions.read()\n",
    "    cleaned_data = pipeline.cleaned_dataset.read()\n",
    "\n",
    "    # Set arbitrarily the time window for the chart as 5 times the number of predictions\n",
    "    window = 5 * n_predictions\n",
    "\n",
    "    # Create the historical dataset that will be displayed\n",
    "    new_length = len(cleaned_data[cleaned_data[\"Date\"] < day]) + n_predictions\n",
    "    temp_df = cleaned_data[:new_length]\n",
    "    temp_df = temp_df[-window:].reset_index(drop=True)\n",
    "\n",
    "    # Create the series that will be used in the concat\n",
    "    historical_values = pd.Series(temp_df[\"Value\"], name=\"Historical values\")\n",
    "    predicted_values = pd.Series([np.NaN]*len(temp_df), name=\"Predicted values\")\n",
    "    predicted_values[-len(predictions):] = predictions\n",
    "\n",
    "    # Create the predictions dataset\n",
    "    # Columns : [Date, Historical values, Predicted values]\n",
    "    return pd.concat([temp_df[\"Date\"], historical_values, predicted_values], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e1cd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_predictions_dataset(state, pipeline):\n",
    "    print(\"Updating predictions dataset...\")\n",
    "    state.predictions_dataset = create_predictions_dataset(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a048bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n",
      "'Predict' button clicked\n",
      "Execution of pipeline...\n",
      "[2023-01-31 15:09:29,572][Taipy][INFO] job JOB_clean_data_e8861791-382d-4935-bb0d-040e82a796d9 is skipped.\n",
      "     Predicting baseline\n",
      "[2023-01-31 15:09:29,606][Taipy][INFO] job JOB_predict_baseline_aa408da9-5152-4ca3-a43f-f02627375aae is completed.\n",
      "Updating predictions dataset...\n",
      "Creating predictions dataset...\n"
     ]
    }
   ],
   "source": [
    "Gui(page=pipeline_page).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56ff9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of clarity, we have used an AutoRegressive model rather than a pure ML model such as:\n",
    "# Random Forest, Linear Regression, LSTM, etc   \n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# This is the function that will be used by the task\n",
    "def predict_ml(cleaned_dataset: pd.DataFrame, n_predictions: int, day: dt.datetime, max_capacity: int):\n",
    "    print(\"     Predicting with ML\")\n",
    "    # Select the train data\n",
    "    train_dataset = cleaned_dataset[cleaned_dataset[\"Date\"] < day]\n",
    "\n",
    "    # Fit the AutoRegressive model\n",
    "    model = AutoReg(train_dataset[\"Value\"], lags=7).fit()\n",
    "\n",
    "    # Get the n_predictions forecasts\n",
    "    predictions = model.forecast(n_predictions).reset_index(drop=True)\n",
    "    predictions = predictions.apply(lambda x: min(x, max_capacity))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f6accb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the list of pipelines names\n",
    "# It will be used in a selector of pipelines\n",
    "pipeline_selector = [\"baseline\", \"ml\"]\n",
    "selected_pipeline = pipeline_selector[0]\n",
    "scenario_page = page + \"\"\"\n",
    "Select the pipeline\n",
    "<|{selected_pipeline}|selector|lov={pipeline_selector}|> <|Update chart|button|on_action=update_chart|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bb091cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario():\n",
    "    print(\"Creating scenario...\")\n",
    "    scenario = tp.create_scenario(scenario_cfg)\n",
    "    scenario = submit_scenario(scenario)\n",
    "    return scenario\n",
    "\n",
    "def submit_scenario(scenario):\n",
    "    print(\"Submitting scenario...\")\n",
    "    tp.submit(scenario)\n",
    "    return scenario\n",
    "\n",
    "def update_chart(state):\n",
    "    print(\"'Update chart' button clicked\")\n",
    "    # Select the right pipeline\n",
    "    pipeline = scenario.pipelines[state.selected_pipeline]\n",
    "\n",
    "    # Update the chart based on this pipeline\n",
    "    # It is the same function as created before in step_5\n",
    "    update_predictions_dataset(state, pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "992c47c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all entities\n",
    "Config.configure_global_app(clean_entities_enabled=True)\n",
    "tp.clean_all_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de86f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scenario...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scenario_cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/16/sj7htwpn37x5chbjt67h8nrr0000gn/T/ipykernel_24235/809716200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creation of our first scenario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mGui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscenario_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdark_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/16/sj7htwpn37x5chbjt67h8nrr0000gn/T/ipykernel_24235/863303106.py\u001b[0m in \u001b[0;36mcreate_scenario\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating scenario...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scenario_cfg' is not defined"
     ]
    }
   ],
   "source": [
    "# Creation of our first scenario\n",
    "scenario = create_scenario()\n",
    "Gui(page=scenario_page).run(dark_mode=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b553e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial variables\n",
    "## Initial variables for the scenario   \n",
    "day = dt.datetime(2021, 7, 26)\n",
    "n_predictions = 40\n",
    "max_capacity = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57e6a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_scenario_manager = page + \"\"\"\n",
    "# Change your scenario\n",
    "\n",
    "**Prediction date**\\n\\n <|{day}|date|not with_time|>\n",
    "\n",
    "**Max capacity**\\n\\n <|{max_capacity}|number|>\n",
    "\n",
    "**Number of predictions**\\n\\n<|{n_predictions}|number|>\n",
    "\n",
    "<|Save changes|button|on_action={submit_scenario}|>\n",
    "\n",
    "Select the pipeline\n",
    "<|{selected_pipeline}|selector|lov={pipeline_selector}|> <|Update chart|button|on_action={update_chart}|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afbad2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario():\n",
    "    global selected_scenario\n",
    "\n",
    "    print(\"Creating scenario...\")\n",
    "    scenario = tp.create_scenario(scenario_cfg)\n",
    "\n",
    "    selected_scenario = scenario.id\n",
    "\n",
    "    tp.submit(scenario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a243d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_scenario(state):\n",
    "    print(\"Submitting scenario...\")\n",
    "    # Get the selected scenario: in this current step a single scenario is created then modified here.\n",
    "    scenario = tp.get(selected_scenario)\n",
    "\n",
    "    # Conversion to the right format\n",
    "    state_day = dt.datetime(state.day.year, state.day.month, state.day.day)\n",
    "\n",
    "    # Change the default parameters by writing in the datanodes\n",
    "    scenario.day.write(state_day)\n",
    "    scenario.n_predictions.write(int(state.n_predictions))\n",
    "    scenario.max_capacity.write(int(state.max_capacity))\n",
    "\n",
    "    # Execute the pipelines/code\n",
    "    tp.submit(scenario)\n",
    "\n",
    "    # Update the chart when we change the scenario\n",
    "    update_chart(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d76cf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scenario...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scenario_cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/16/sj7htwpn37x5chbjt67h8nrr0000gn/T/ipykernel_24235/4200081808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mselected_scenario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Creation of a single scenario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcreate_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mGui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage_scenario_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdark_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/16/sj7htwpn37x5chbjt67h8nrr0000gn/T/ipykernel_24235/2814778042.py\u001b[0m in \u001b[0;36mcreate_scenario\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating scenario...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mselected_scenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scenario_cfg' is not defined"
     ]
    }
   ],
   "source": [
    "def update_chart(state):\n",
    "    # Select the right scenario and pipeline\n",
    "    scenario = tp.get(selected_scenario)\n",
    "    pipeline = scenario.pipelines[state.selected_pipeline]\n",
    "    # Update the chart based on this pipeline\n",
    "    update_predictions_dataset(state, pipeline)\n",
    "\n",
    "\n",
    "global selected_scenario\n",
    "# Creation of a single scenario\n",
    "create_scenario()\n",
    "Gui(page=page_scenario_manager).run(dark_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085dfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
