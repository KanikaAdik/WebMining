{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW #1: Analyze Documents by Numpy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirements (shown as bullets) . In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n",
    "- Follow the Submission Instruction to submit your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "In this assignment, you'll write functions to analyze an article to find out the word distributions and key concepts. \n",
    "\n",
    "The packages you'll need for this assignment include `numpy` and `string`. Some useful functions:\n",
    "- string, list, dictionary: `split`, `count`, `index`,`strip`\n",
    "- numpy: `sum`, `where`,`log`, `argsort`,`argmin`, `argmax` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in an input sentence\n",
    "\n",
    "\n",
    "Define a function named `tokenize(doc)` which does the following: \n",
    "\n",
    "* accepts a document (i.e., `doc` parameter) as an input\n",
    "* first splits a document into paragraphs by delimiter `\\n\\n` (i.e. two new lines)\n",
    "* for each paragraph, \n",
    "    - splits it into a list of tokens by **space** (including tab, and new line). \n",
    "        - e.g., `it's a hello world!!!` will be split into tokens `[\"it's\", \"a\",\"hello\",\"world!!!\"]`  \n",
    "    - removes the **leading/trailing punctuations or spaces** of each token, if any \n",
    "        - e.g., `world!!! -> world`, while `it's` does not change\n",
    "        - hint, you can import module *string*, use `string.punctuation` to get a list of punctuations (say `puncts`), and then use function `strip(puncts)` to remove leading or trailing punctuations in each token\n",
    "    - a token has at least two characters  \n",
    "    - converts all tokens into lower case \n",
    "    - find the count of each unique token and save the count as a dictionary, named `word_dict`, i.e., `{world: 1, a: 1, ...}` \n",
    "* creates another dictionary,say `para_dict`, where a key is the order of each paragraph in `doc`, and the value is the `word_dict` generated from this paragraph\n",
    "* returns the dictionary `para_dict` and the paragraphs in the document\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"it's\", 'a', 'hello', 'world']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"it's\", 'a', 'hello', 'world']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0, \"it's\"), (1, 'a'), (2, 'hello'), (3, 'world')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"it's\", 'a', 'hello', 'world!!!', 'it', 'is', 'hello', 'world', 'again.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "puncts = string.punctuation\n",
    "puncts\n",
    "para = \"it's a hello world!!!\"\n",
    "tokens = para.split(\" \")\n",
    "import re\n",
    "tokens = [re.sub('[^a-zA-Z0-9 \\n\\.\\']','',each.strip()) for each in tokens]\n",
    "tokens\n",
    "tokens1 = [each.strip(puncts) for each in tokens]\n",
    "tokens1 \n",
    "\n",
    "t =[(i, v) for i,v in enumerate(tokens1)]\n",
    "t\n",
    "a = \"it's a hello world!!!\\nit is hello world again.\"\n",
    "d = a.split()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):#accepts a document (i.e., doc parameter) as an input\n",
    "    \n",
    "    para_dict, para = None, None\n",
    "    \n",
    "    # add your code here\n",
    "    #first splits a document into paragraphs by delimiter \\n\\n (i.e. two new lines)\n",
    "    para = doc.split('\\n\\n')\n",
    "    #for each paragraph, splits it into a list of tokens by space (including tab, and new line).\n",
    "    #e.g., \"it's a hello world!!! will be split into tokens [\"it's\", \"a\",\"hello\",\"world!!!\"]\n",
    "    import string\n",
    "    puncts = string.punctuation\n",
    "    #find the count of each unique token and save the count as a dictionary, named word_dict, i.e., {world: 1, a: 1, ...}\n",
    "    def get_tokens(para): \n",
    "        tokens = para.split()\n",
    "        tokens = [each.strip(puncts) for each in tokens]\n",
    "        tokens = [token.lower() for token in tokens if len(token)>=2]\n",
    "        word_dict={}\n",
    "        for a_token in tokens:\n",
    "            if a_token not in word_dict.keys():\n",
    "                word_dict[a_token]=1\n",
    "            else:\n",
    "                word_dict[a_token] +=1\n",
    "        return word_dict\n",
    "    \n",
    "    para_dict={}\n",
    "    #creates another dictionary,say para_dict, where a key is the order of each paragraph in doc, \n",
    "    #and the value is the word_dict generated from this paragraph\n",
    "    para_dict = {i : get_tokens(v) for i,v in enumerate(para)}\n",
    "    #returns the dictionary para_dict and the paragraphs in the document\n",
    "    return para_dict, para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: {\"it's\": 1, 'hello': 2, 'world': 2, 'it': 1, 'is': 1, 'again': 1},\n",
       "  1: {'this': 1, 'is': 1, 'paragraph': 1}},\n",
       " [\"it's a hello world!!!\\nit is hello world again.\", 'This is paragraph 2.'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "doc = \"it's a hello world!!!\\nit is hello world again.\\n\\nThis is paragraph 2.\"\n",
    "tokenize(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Generate a document term matrix (DTM) as a numpy array\n",
    "\n",
    "\n",
    "Define a function `get_dtm(doc)` as follows:\n",
    "- accepts a document, i.e., `doc`, as an input\n",
    "- uses `tokenize` function you defined in Q1 to get the word dictionary for each paragraph in the document \n",
    "- pools the keys from all the word dictionaries to get a list of  unique words, denoted as `unique_words` \n",
    "- creates a numpy array, say `dtm` with a shape (# of paragraphs x # of unique words), and set the initial values to 0. \n",
    "- fills cell `dtm[i,j]` with the count of the `j`th word in the `i`th paragraph \n",
    "- returns `dtm` and `unique_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtm(doc):\n",
    "    dtm, all_words = None, None\n",
    "    dtm=[]\n",
    "    all_words=[]\n",
    "    # add your code here\n",
    "    #uses tokenize function to get the word dictionary for each paragraph in the document\n",
    "    para_dict, para = tokenize(doc)\n",
    "    #pools the keys from all the word dictionaries to get a list of unique words\n",
    "    for para_tokens in para_dict.values():\n",
    "        [all_words.append(token) for token in para_tokens if token not in all_words]\n",
    "    #creates a numpy array, say dtm with a shape (# of paragraphs x # of unique words), and set the initial values to 0.\n",
    "    dtm = np.zeros((len(para), len(all_words)))\n",
    "    #fills cell dtm[i,j] with the count of the jth word in the ith paragraph\n",
    "    for i, para_num in enumerate(para_dict):\n",
    "        for j, token in enumerate(all_words):\n",
    "            if token in para_dict[para_num]:\n",
    "                dtm[i][j] =para_dict[i][token]\n",
    "    return dtm, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: {\"it's\": 1, 'hello': 2, 'world': 2, 'it': 1, 'is': 1, 'again': 1},\n",
       "  1: {'this': 1, 'is': 1, 'paragraph': 1}},\n",
       " [\"it's a hello world!!!\\nit is hello world again.\", 'This is paragraph 2.'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 2., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"it's\", 'hello', 'world', 'it', 'is', 'again', 'this', 'paragraph']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"it's a hello world!!!\\nit is hello world again.\\n\\nThis is paragraph 2.\"\n",
    "tokenize(doc)\n",
    "dtm, all_words = get_dtm(doc)\n",
    "dtm\n",
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A test document. This document can be found at https://www.wboi.org/npr-news/2023-01-26/everybody-is-cheating-why-this-teacher-has-adopted-an-open-chatgpt-policy\n",
    "\n",
    "doc = open(\"chatgpt_npr.txt\", 'r').read()\n",
    "dtm, all_words = get_dtm(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = open(\"chatgpt_npr.txt\", 'r').read() \n",
    "dtm, all_words = get_dtm(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethan Mollick has a message for the humans and the machines: can't we all just get along?\n",
      "\n",
      "After all, we are now officially in an A.I. world and we're going to have to share it, reasons the associate professor at the University of Pennsylvania's prestigious Wharton School.\n",
      "\n",
      "\"This was a sudden change, right? There is a lot of good stuff that we are going to have to do differently, but I think we could solve the problems of how we teach people to write in a world with ChatGPT,\" Mollick told NPR.\n",
      "\n",
      "Ever since the chatbot ChatGPT launched in November, educators have raised concerns it could facilitate cheating.\n",
      "\n",
      "Some school districts have banned access to the bot, and not without reason. The artificial intelligence tool from the company OpenAI can compose poetry. It can write computer code. It can maybe even pass an MBA exam.\n",
      "\n",
      "One Wharton professor recently fed the chatbot the final exam questions for a core MBA course and found that, despite some surprising math errors, he would have given it a B or a B-minus in the class.\n",
      "\n",
      "And yet, not all educators are shying away from the bot.\n",
      "\n",
      "This year, Mollick is not only allowing his students to use ChatGPT, they are required to. And he has formally adopted an A.I. policy into his syllabus for the first time.\n",
      "\n",
      "He teaches classes in entrepreneurship and innovation, and said the early indications were the move was going great.\n",
      "\n",
      "\"The truth is, I probably couldn't have stopped them even if I didn't require it,\" Mollick said.\n",
      "\n",
      "This week he ran a session where students were asked to come up with ideas for their class project. Almost everyone had ChatGPT running and were asking it to generate projects, and then they interrogated the bot's ideas with further prompts.\n",
      "\n",
      "\"And the ideas so far are great, partially as a result of that set of interactions,\" Mollick said.\n",
      "\n",
      " Users experimenting with the chatbot are warned before testing the tool that ChatGPT \"may occasionally generate incorrect or misleading information.\"\n",
      "/ OpenAI/Screenshot By NPR\n",
      "/\n",
      "OpenAI/Screenshot By NPR\n",
      "Users experimenting with the chatbot are warned before testing the tool that ChatGPT \"may occasionally generate incorrect or misleading information.\"\n",
      "He readily admits he alternates between enthusiasm and anxiety about how artificial intelligence can change assessments in the classroom, but he believes educators need to move with the times.\n",
      "\n",
      "\"We taught people how to do math in a world with calculators,\" he said. Now the challenge is for educators to teach students how the world has changed again, and how they can adapt to that.\n",
      "\n",
      "Mollick's new policy states that using A.I. is an \"emerging skill\"; that it can be wrong and students should check its results against other sources; and that they will be responsible for any errors or omissions provided by the tool.\n",
      "\n",
      "And, perhaps most importantly, students need to acknowledge when and how they have used it.\n",
      "\n",
      "\"Failure to do so is in violation of academic honesty policies,\" the policy reads.\n",
      "\n",
      "Mollick isn't the first to try to put guardrails in place for a post-ChatGPT world.\n",
      "\n",
      "Earlier this month, 22-year-old Princeton student Edward Tian created an app to detect if something had been written by a machine. Named GPTZero, it was so popular that when he launched it, the app crashed from overuse.\n",
      "\n",
      "\"Humans deserve to know when something is written by a human or written by a machine,\" Tian told NPR of his motivation.\n",
      "\n",
      "Mollick agrees, but isn't convinced that educators can ever truly stop cheating.\n",
      "\n",
      "He cites a survey of Stanford students that found many had already used ChatGPT in their final exams, and he points to estimates that thousands of people in places like Kenya are writing essays on behalf of students abroad.\n",
      "\n",
      "\"I think everybody is cheating ... I mean, it's happening. So what I'm asking students to do is just be honest with me,\" he said. \"Tell me what they use ChatGPT for, tell me what they used as prompts to get it to do what they want, and that's all I'm asking from them. We're in a world where this is happening, but now it's just going to be at an even grander scale.\"\n",
      "\n",
      "\"I don't think human nature changes as a result of ChatGPT. I think capability did.\"\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'all',\n",
       " 'are',\n",
       " 'educators',\n",
       " 'bot',\n",
       " 'not',\n",
       " 'from',\n",
       " 'yet',\n",
       " 'shying',\n",
       " 'away']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To ensure dtm is correct, check what words in a paragraph have been captured by dtm\n",
    "\n",
    "p = 6 # paragraph id\n",
    "\n",
    "[w for i,w in enumerate(all_words) if dtm[p][i]>0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Analyze DTM Array \n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words, paragraphs)` which:\n",
    "* takes an array $dtm$ and $words$ as an input, where $dtm$ is the array you get in Q2 with a shape $(m \\times n)$, and $words$ contains an array of words corresponding to the columns of $dtm$.\n",
    "* calculates the paragraph frequency for each word $j$, e.g. how many paragraphs contain word $j$. Save the result to array $df$. $df$ has shape of $(n,)$ or $(1, n)$. \n",
    "* normalizes the word count per paragraph: divides word count, i.e., $dtm_{i,j}$, by the total number of words in paragraph $i$. Save the result as an array named $tf$. $tf$ has shape of $(m,n)$. \n",
    "* for each $dtm_{i,j}$, calculates $tfidf_{i,j} = \\frac{tf_{i, j}}{1+log(df_j)}$, i.e., divide each normalized word count by the log of the paragraph frequency of the word (add 1 to the denominator to avoid dividing by 0).  $tfidf$ has shape of $(m,n)$ \n",
    "* prints out the following (hint: you can zip words and their values into a list so that there is no need for loop during printing):\n",
    "    \n",
    "    - the total number of words in the document represented by $dtm$ \n",
    "    - the number of paragraphs and the number of unique words in the document\n",
    "    - the most frequent top 10 words in this document    \n",
    "    - top-10 words that show in most of the paragraphs, i.e. words with the top 10 largest $df$ values (show words and their $df$ values) \n",
    "    - the shortest paragraph (i.e., the one with the least number of words) \n",
    "    - top-5 words with the largest $tfidf$ values in the longest sentence (show words and values) \n",
    "\n",
    "Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation.\n",
    "\n",
    "Your answer may be different from the example output, since words may have the same values in the dtm but are kept in positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dtm(dtm, words, paragraphs):\n",
    "    \n",
    "    # add your code here\n",
    "    # calculates the paragraph frequency for each word  𝑗, e.g. how many paragraphs contain word  𝑗. Save the result to array  𝑑𝑓. 𝑑𝑓 has shape of  (𝑛,) or  (1,𝑛)\n",
    "    frequency = np.sum(dtm, axis=0) \n",
    "    dtm1 = np.where(dtm, 1, 0)\n",
    "    df = np.sum(dtm1, axis=0) #paragraph frequency for each word  \n",
    "    #print(\"shape of dtm - \", dtm.shape, dtm1.shape)\n",
    "    #print(\"shape of df - \", df.shape)\n",
    "    tf = np.divide(dtm, np.sum(dtm, axis=1, keepdims = True))#normalizes the word count per paragraph: divides word count, i.e.,  𝑑𝑡𝑚𝑖,𝑗  by the total number of words in paragraph  𝑖\n",
    "    #print(\"tf shape - \", tf.shape)\n",
    "    tfidf = tf/(1+np.log(df)) #for each  𝑑𝑡𝑚𝑖,𝑗 , calculates  𝑡𝑓𝑖𝑑𝑓𝑖,𝑗=𝑡𝑓𝑖,𝑗1+𝑙𝑜𝑔(𝑑𝑓𝑗)\n",
    "    #print(\"tfIDF shape - \", tfidf.shape)\n",
    "    \n",
    "    fx = frequency.argsort()\n",
    "    dfx = df.argsort()\n",
    "    top10_frequencies = frequency[fx][:-10:-1]\n",
    "    top10_words = words[fx][:-10:-1]\n",
    "    top10_df_values = df[dfx][:-10:-1]\n",
    "    top10_df_words = words[dfx][:-10:-1]\n",
    "    \n",
    "    #shortest paragraph --\n",
    "    para_words = np.sum(dtm, axis=1)\n",
    "    location_shortpara = np.argsort(para_words)[0]\n",
    "    location_longpara = np.argsort(para_words)[-1]\n",
    "    #longest paragraph -- \n",
    "    tfidf_values = tfidf[location_longpara]\n",
    "    tfidfx  = np.argsort(tfidf_values)\n",
    "    \n",
    "    top5_tfidf_values = tfidf_values[tfidfx]\n",
    "    top5_words = words[tfidfx]\n",
    "    \n",
    "    print(\"\\nThe total number of words:\\n\",np.sum(dtm)) #the total number of words in the document represented by  𝑑𝑡𝑚\n",
    "    print(\"\\nthe number of paragraphs:\", dtm.shape[0],\", the number of unique words in the document: \",dtm.shape[1])  #the number of paragraphs and the number of unique words in the document\n",
    "    print(\"\\nThe top 10 frequent words:\\n\", list(zip(top10_words, top10_frequencies))) #the most frequent top 10 words in this document\n",
    "    print(\"\\nThe top 10 words with highest df values:\\n\", list(zip(top10_df_words, top10_df_values)))\n",
    "    print(\"\\nThe shortest paragraph :\\n\", paragraphs[location_shortpara])\n",
    "    print(\"\\nThe top 5 words with the highest tf-idf values in the longest paragraph:\\n\",list(zip(top5_words[-5:],top5_tfidf_values[-5:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The total number of words:\n",
      " 686.0\n",
      "\n",
      "the number of paragraphs: 24 , the number of unique words in the document:  312\n",
      "\n",
      "The top 10 frequent words:\n",
      " [('the', 31.0), ('to', 25.0), ('and', 19.0), ('that', 13.0), ('it', 12.0), ('he', 12.0), ('in', 12.0), ('of', 11.0), ('is', 10.0)]\n",
      "\n",
      "The top 10 words with highest df values:\n",
      " [('the', 18), ('and', 15), ('to', 14), ('in', 11), ('it', 10), ('he', 9), ('that', 9), ('is', 8), ('for', 8)]\n",
      "\n",
      "The shortest paragraph :\n",
      " And yet, not all educators are shying away from the bot.\n",
      "\n",
      "The top 5 words with the highest tf-idf values in the longest paragraph:\n",
      " [('incorrect', 0.02666666666666667), ('misleading', 0.02666666666666667), ('information', 0.02666666666666667), ('warned', 0.02666666666666667), ('openai/screenshot', 0.02666666666666667)]\n"
     ]
    }
   ],
   "source": [
    "para_dict, paragraphs = tokenize(doc)\n",
    "\n",
    "# convert words in numpy arrays so that you can use array slicing\n",
    "words = np.array(all_words)\n",
    "\n",
    "analyze_dtm(dtm, words, paragraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Find co-occuring words (Bonus) \n",
    "\n",
    "Can you leverage $dtm$ array you generated to find what words frequently coocur with a specific word? For example, \"students\" and \"chatgpt\" coocur in 4 paragraphs. \n",
    "\n",
    "Define a function `find_coocur(w1, w2, dtm, words)`, which returns the paragraphs containong both words w1 and w2.\n",
    "\n",
    "Use a pdf file to describe your ideas and also implement your ideas. Again, `DO NOT USE LOOP`! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coocur(w1, w2, dtm, words, paragraphs):\n",
    "    \n",
    "    result = []\n",
    "    # add your code here\n",
    "    # find para location having both words\n",
    "    w1_index = np.where(words == w1)[0]\n",
    "    w2_index = np.where(words == w2)[0]\n",
    "    \n",
    "    w1_dtm = dtm[:,w1_index]\n",
    "    w2_dtm = dtm[:,w2_index]\n",
    "    \n",
    "    para_position = np.where((w1_dtm!=0) & (w2_dtm!=0))[0]\n",
    "\n",
    "    result = np.array(paragraphs)[para_position]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['This year, Mollick is not only allowing his students to use ChatGPT, they are required to. And he has formally adopted an A.I. policy into his syllabus for the first time.',\n",
       "       \"This week he ran a session where students were asked to come up with ideas for their class project. Almost everyone had ChatGPT running and were asking it to generate projects, and then they interrogated the bot's ideas with further prompts.\",\n",
       "       'He cites a survey of Stanford students that found many had already used ChatGPT in their final exams, and he points to estimates that thousands of people in places like Kenya are writing essays on behalf of students abroad.',\n",
       "       '\"I think everybody is cheating ... I mean, it\\'s happening. So what I\\'m asking students to do is just be honest with me,\" he said. \"Tell me what they use ChatGPT for, tell me what they used as prompts to get it to do what they want, and that\\'s all I\\'m asking from them. We\\'re in a world where this is happening, but now it\\'s just going to be at an even grander scale.\"'],\n",
       "      dtype='<U547')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = find_coocur('chatgpt','students',dtm, words, paragraphs)\n",
    "len(ps)\n",
    "ps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put everything together and test using main block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question 1\n",
      "({0: {\"it's\": 1, 'hello': 2, 'world': 2, 'it': 1, 'is': 1, 'again': 1}, 1: {'this': 1, 'is': 1, 'paragraph': 1}}, [\"it's a hello world!!!\\nit is hello world again.\", 'This is paragraph 2.'])\n",
      "\n",
      "Test Question 2\n",
      "[[1. 2. 2. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 1.]]\n",
      "[\"it's\", 'hello', 'world', 'it', 'is', 'again', 'this', 'paragraph']\n",
      "\n",
      "Test Question 3\n",
      "\n",
      "The total number of words:\n",
      " 686.0\n",
      "\n",
      "the number of paragraphs: 24 , the number of unique words in the document:  312\n",
      "\n",
      "The top 10 frequent words:\n",
      " [('the', 31.0), ('to', 25.0), ('and', 19.0), ('that', 13.0), ('it', 12.0), ('he', 12.0), ('in', 12.0), ('of', 11.0), ('is', 10.0)]\n",
      "\n",
      "The top 10 words with highest df values:\n",
      " [('the', 18), ('and', 15), ('to', 14), ('in', 11), ('it', 10), ('he', 9), ('that', 9), ('is', 8), ('for', 8)]\n",
      "\n",
      "The shortest paragraph :\n",
      " And yet, not all educators are shying away from the bot.\n",
      "\n",
      "The top 5 words with the highest tf-idf values in the longest paragraph:\n",
      " [('incorrect', 0.02666666666666667), ('misleading', 0.02666666666666667), ('information', 0.02666666666666667), ('warned', 0.02666666666666667), ('openai/screenshot', 0.02666666666666667)]\n"
     ]
    }
   ],
   "source": [
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    doc = \"it's a hello world!!!\\nit is hello world again.\\n\\nThis is paragraph 2.\"\n",
    "    print(\"Test Question 1\")\n",
    "    para_dict, paragraphs = tokenize(doc)\n",
    "    print(tokenize(doc))\n",
    "    \n",
    "    \n",
    "    # Test Question 2\n",
    "    print(\"\\nTest Question 2\")\n",
    "    dtm, all_words = get_dtm(doc)\n",
    "\n",
    "    print(dtm)\n",
    "    print(all_words)\n",
    "    \n",
    "    \n",
    "    #3 Test Question 3\n",
    "    \n",
    "    doc = open(\"chatgpt_npr.txt\", 'r').read()\n",
    "    \n",
    "    para_dict, paragraphs = tokenize(doc)\n",
    "    dtm, all_words = get_dtm(doc)\n",
    "\n",
    "    print(\"\\nTest Question 3\")\n",
    "    words = np.array(all_words)\n",
    "\n",
    "    tfidf = analyze_dtm(dtm, words, paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
