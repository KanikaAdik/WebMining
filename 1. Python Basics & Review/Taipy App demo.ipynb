{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce45c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting taipy\n",
      "  Downloading taipy-2.0.0-py3-none-any.whl (8.1 kB)\n",
      "Collecting taipy-gui<2.1,>=2.0\n",
      "  Downloading taipy_gui-2.0.4-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting taipy-rest<2.1,>=2.0\n",
      "  Downloading taipy_rest-2.0.0-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pytz<2022.2,>=2021.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-gui<2.1,>=2.0->taipy) (2021.3)\n",
      "Collecting flask<2.3,>=2.2\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 11.4 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pandas<2.0,>=1.4.4\n",
      "  Downloading pandas-1.5.3-cp39-cp39-macosx_10_9_x86_64.whl (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gevent-websocket<0.11,>=0.10.1\n",
      "  Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
      "Collecting flask-socketio<6.0,>=5.3.0\n",
      "  Downloading Flask_SocketIO-5.3.2-py3-none-any.whl (17 kB)\n",
      "Collecting markdown<4.0,>=3.4.1\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting taipy-config<3.0,>=2.0\n",
      "  Downloading taipy_config-2.1.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting kthread<0.3,>=0.2.3\n",
      "  Downloading kthread-0.2.3-py3-none-any.whl (3.9 kB)\n",
      "Collecting flask-cors<4.0,>=3.0.10\n",
      "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting tzlocal<5.0,>=3.0\n",
      "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting python-dotenv<0.21,>=0.19\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Collecting gevent<22.0,>=21.12.0\n",
      "  Downloading gevent-21.12.0-cp39-cp39-macosx_10_14_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 23.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (4.8.1)\n",
      "Requirement already satisfied: Six in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask-cors<4.0,>=3.0.10->taipy-gui<2.1,>=2.0->taipy) (1.16.0)\n",
      "Collecting python-socketio>=5.0.2\n",
      "  Downloading python_socketio-5.7.2-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zope.interface in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (58.0.4)\n",
      "Requirement already satisfied: zope.event in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (4.5.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from gevent<22.0,>=21.12.0->taipy-gui<2.1,>=2.0->taipy) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask<2.3,>=2.2->taipy-gui<2.1,>=2.0->taipy) (3.6.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp39-cp39-macosx_10_9_x86_64.whl (13 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0,>=1.4.4->taipy-gui<2.1,>=2.0->taipy) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0,>=1.4.4->taipy-gui<2.1,>=2.0->taipy) (2.8.2)\n",
      "Collecting python-engineio>=4.3.0\n",
      "  Downloading python_engineio-4.3.4-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting bidict>=0.21.0\n",
      "  Downloading bidict-0.22.1-py3-none-any.whl (35 kB)\n",
      "Collecting deepdiff<6.3,>=6.2\n",
      "  Downloading deepdiff-6.2.3-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml<0.11,>=0.10 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-config<3.0,>=2.0->taipy-gui<2.1,>=2.0->taipy) (0.10.2)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.5-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (489 kB)\n",
      "\u001b[K     |████████████████████████████████| 489 kB 20.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ordered-set<4.2.0,>=4.0.2\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting passlib<1.8,>=1.7.4\n",
      "  Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
      "\u001b[K     |████████████████████████████████| 525 kB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marshmallow-sqlalchemy<0.29,>=0.25\n",
      "  Downloading marshmallow_sqlalchemy-0.28.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting apispec[yaml]<6.0,>=5.1\n",
      "  Downloading apispec-5.2.2-py3-none-any.whl (29 kB)\n",
      "Collecting flask-migrate<4.0,>=3.1\n",
      "  Downloading Flask_Migrate-3.1.0-py3-none-any.whl (20 kB)\n",
      "Collecting flask-jwt-extended<5.0,>=4.3\n",
      "  Downloading Flask_JWT_Extended-4.4.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting taipy-core<2.1,>=2.0\n",
      "  Downloading taipy_core-2.0.4-py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flask-marshmallow<0.15,>=0.14\n",
      "  Downloading flask_marshmallow-0.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting flask-restful<0.4,>=0.3.9\n",
      "  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "Collecting apispec-webframeworks<0.6,>=0.5.2\n",
      "  Downloading apispec_webframeworks-0.5.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from apispec[yaml]<6.0,>=5.1->taipy-rest<2.1,>=2.0->taipy) (6.0)\n",
      "Requirement already satisfied: PyJWT<3.0,>=2.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from flask-jwt-extended<5.0,>=4.3->taipy-rest<2.1,>=2.0->taipy) (2.1.0)\n",
      "Collecting marshmallow>=2.0.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic>=0.7\n",
      "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 28.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Flask-SQLAlchemy>=1.0\n",
      "  Downloading Flask_SQLAlchemy-3.0.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.3.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from alembic>=0.7->flask-migrate<4.0,>=3.1->taipy-rest<2.1,>=2.0->taipy) (1.4.22)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aniso8601>=0.82\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from marshmallow>=2.0.0->flask-marshmallow<0.15,>=0.14->taipy-rest<2.1,>=2.0->taipy) (21.0)\n",
      "Collecting packaging>=17.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 7.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting taipy-config<3.0,>=2.0\n",
      "  Downloading taipy_config-2.0.1-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: openpyxl<4.0,>=3.0.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-core<2.1,>=2.0->taipy-rest<2.1,>=2.0->taipy) (3.0.9)\n",
      "Requirement already satisfied: networkx<3.0,>=2.6 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from taipy-core<2.1,>=2.0->taipy-rest<2.1,>=2.0->taipy) (2.6.3)\n",
      "Requirement already satisfied: et-xmlfile in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from openpyxl<4.0,>=3.0.3->taipy-core<2.1,>=2.0->taipy-rest<2.1,>=2.0->taipy) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
      "\u001b[K     |████████████████████████████████| 340 kB 43.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: MarkupSafe, Werkzeug, Jinja2, tzdata, python-engineio, packaging, Mako, flask, bidict, apispec, taipy-config, pytz-deprecation-shim, python-socketio, pandas, marshmallow, gevent, Flask-SQLAlchemy, aniso8601, alembic, tzlocal, taipy-core, python-dotenv, passlib, marshmallow-sqlalchemy, markdown, kthread, gevent-websocket, flask-socketio, flask-restful, flask-migrate, flask-marshmallow, flask-jwt-extended, flask-cors, apispec-webframeworks, taipy-rest, taipy-gui, taipy\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.0.2\n",
      "    Uninstalling Werkzeug-2.0.2:\n",
      "      Successfully uninstalled Werkzeug-2.0.2\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.0\n",
      "    Uninstalling packaging-21.0:\n",
      "      Successfully uninstalled packaging-21.0\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 1.1.2\n",
      "    Uninstalling Flask-1.1.2:\n",
      "      Successfully uninstalled Flask-1.1.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\n",
      "  Attempting uninstall: gevent\n",
      "    Found existing installation: gevent 21.8.0\n",
      "    Uninstalling gevent-21.8.0:\n",
      "      Successfully uninstalled gevent-21.8.0\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.3.7\n",
      "    Uninstalling Markdown-3.3.7:\n",
      "      Successfully uninstalled Markdown-3.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "cookiecutter 1.7.2 requires Jinja2<3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
      "cookiecutter 1.7.2 requires MarkupSafe<2.0.0, but you have markupsafe 2.1.2 which is incompatible.\u001b[0m\n",
      "Successfully installed Flask-SQLAlchemy-3.0.3 Jinja2-3.1.2 Mako-1.2.4 MarkupSafe-2.1.2 Werkzeug-2.2.2 alembic-1.9.2 aniso8601-9.0.1 apispec-5.2.2 apispec-webframeworks-0.5.2 bidict-0.22.1 flask-2.2.2 flask-cors-3.0.10 flask-jwt-extended-4.4.4 flask-marshmallow-0.14.0 flask-migrate-3.1.0 flask-restful-0.3.9 flask-socketio-5.3.2 gevent-21.12.0 gevent-websocket-0.10.1 kthread-0.2.3 markdown-3.4.1 marshmallow-3.19.0 marshmallow-sqlalchemy-0.28.1 packaging-23.0 pandas-1.5.3 passlib-1.7.4 python-dotenv-0.20.0 python-engineio-4.3.4 python-socketio-5.7.2 pytz-deprecation-shim-0.1.0.post0 taipy-2.0.0 taipy-config-2.0.1 taipy-core-2.0.4 taipy-gui-2.0.4 taipy-rest-2.0.0 tzdata-2022.7 tzlocal-4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install taipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12076e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdeaa5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: scipy>=1.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (1.7.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.21 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from statsmodels) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.21->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.21->statsmodels) (2021.3)\n",
      "Requirement already satisfied: six in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from patsy>=0.5->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb4410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89adb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "from taipy import Gui\n",
    "Gui(page=\"# Getting started with *Taipy*\").run(dark_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0400aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data(path_to_csv: str):\n",
    "    # pandas.read_csv() returns a pd.DataFrame\n",
    "    dataset = pd.read_csv(path_to_csv)\n",
    "    dataset[\"Date\"] = pd.to_datetime(dataset[\"Date\"])\n",
    "    return dataset\n",
    "\n",
    "# Read the dataframe\n",
    "path_to_csv = \"dataset.csv\"\n",
    "dataset = get_data(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ef7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "from taipy import Gui\n",
    "\n",
    "dataset = get_data(path_to_csv)\n",
    "\n",
    "# Initial value\n",
    "n_week = 10\n",
    "\n",
    "# Definition of the page\n",
    "page = \"\"\"\n",
    "# Getting started with Taipy\n",
    "\n",
    "Week number: *<|{n_week}|>*\n",
    "\n",
    "Interact with this slider to change the week number:\n",
    "<|{n_week}|slider|min=1|max=52|>\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "Display the last three months of data:\n",
    "<|{dataset[9000:]}|chart|type=bar|x=Date|y=Value|height=100%|>\n",
    "\n",
    "<|{dataset}|table|height=400px|width=95%|>\n",
    "\"\"\"\n",
    "\n",
    "# Create a Gui object with our page content\n",
    "Gui(page=page).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7ce279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# Select the week based on the slider value\n",
    "dataset_week = dataset[dataset[\"Date\"].dt.isocalendar().week == n_week]\n",
    "\n",
    "page = \"\"\"\n",
    "# Getting started with Taipy\n",
    "\n",
    "Select week: *<|{n_week}|>*\n",
    "\n",
    "<|{n_week}|slider|min=1|max=52|>\n",
    "\n",
    "<|{dataset_week}|chart|type=bar|x=Date|y=Value|height=100%|width=100%|>\n",
    "\"\"\"\n",
    "\n",
    "# on_change is the function that is called when any variable is changed\n",
    "def on_change(state, var_name: str, var_value):\n",
    "    if var_name == \"n_week\":\n",
    "        # Update the dataset when the slider is moved\n",
    "        state.dataset_week = dataset[dataset[\"Date\"].dt.isocalendar().week == var_value]\n",
    "\n",
    "Gui(page=page).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48e137eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from taipy import Config, Scope\n",
    "\n",
    "## Input Data Nodes\n",
    "initial_dataset_cfg = Config.configure_data_node(id=\"initial_dataset\",\n",
    "                                                 storage_type=\"csv\",\n",
    "                                                 path=path_to_csv,\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "\n",
    "# We assume the current day is the 26th of July 2021.\n",
    "# This day can be changed to simulate multiple executions of scenarios on different days\n",
    "day_cfg = Config.configure_data_node(id=\"day\", default_data=dt.datetime(2021, 7, 26))\n",
    "\n",
    "n_predictions_cfg = Config.configure_data_node(id=\"n_predictions\", default_data=40)\n",
    "\n",
    "max_capacity_cfg = Config.configure_data_node(id=\"max_capacity\", default_data=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e818ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from taipy import Config, Scope\n",
    "\n",
    "## Input Data Nodes\n",
    "initial_dataset_cfg = Config.configure_data_node(id=\"initial_dataset\",\n",
    "                                                 storage_type=\"csv\",\n",
    "                                                 path=path_to_csv,\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "\n",
    "# We assume the current day is the 26th of July 2021.\n",
    "# This day can be changed to simulate multiple executions of scenarios on different days\n",
    "day_cfg = Config.configure_data_node(id=\"day\", default_data=dt.datetime(2021, 7, 26))\n",
    "\n",
    "n_predictions_cfg = Config.configure_data_node(id=\"n_predictions\", default_data=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee3a7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remaining Data Nodes\n",
    "cleaned_dataset_cfg = Config.configure_data_node(id=\"cleaned_dataset\",\n",
    "                                             cacheable=True,\n",
    "                                             validity_period=dt.timedelta(days=1),\n",
    "                                             scope=Scope.GLOBAL) \n",
    "\n",
    "predictions_cfg = Config.configure_data_node(id=\"predictions\", scope=Scope.PIPELINE)\n",
    "\n",
    "def clean_data(initial_dataset: pd.DataFrame):\n",
    "    print(\"     Cleaning data\")\n",
    "    # Convert the date column to datetime\n",
    "    initial_dataset[\"Date\"] = pd.to_datetime(initial_dataset[\"Date\"])\n",
    "    cleaned_dataset = initial_dataset.copy()\n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def predict_baseline(cleaned_dataset: pd.DataFrame, n_predictions: int, day: dt.datetime, max_capacity: int):\n",
    "    print(\"     Predicting baseline\")\n",
    "    # Select the train data\n",
    "    train_dataset = cleaned_dataset[cleaned_dataset[\"Date\"] < day]\n",
    "\n",
    "    predictions = train_dataset[\"Value\"][-n_predictions:].reset_index(drop=True)\n",
    "    predictions = predictions.apply(lambda x: min(x, max_capacity))\n",
    "    return predictions\n",
    "\n",
    "clean_data_task_cfg = Config.configure_task(id=\"clean_data\",\n",
    "                                            function=clean_data,\n",
    "                                            input=initial_dataset_cfg,\n",
    "                                            output=cleaned_dataset_cfg)\n",
    "\n",
    "predict_baseline_task_cfg = Config.configure_task(id=\"predict_baseline\",\n",
    "                                                  function=predict_baseline,\n",
    "                                                  input=[cleaned_dataset_cfg, n_predictions_cfg, day_cfg, max_capacity_cfg],\n",
    "                                                  output=predictions_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48d8ab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Cleaning data\n",
      "[2023-01-31 15:35:16,215][Taipy][INFO] job JOB_clean_data_37a2611a-72f8-401b-9df6-8196d5dd385e is completed.\n",
      "     Predicting baseline\n",
      "[2023-01-31 15:35:16,246][Taipy][INFO] job JOB_predict_baseline_132ac1dd-a1b5-4c7d-bc2c-96eab7785eff is completed.\n",
      "Predictions of baseline algorithm\n",
      " 0     128\n",
      "1     108\n",
      "2     106\n",
      "3      99\n",
      "4      84\n",
      "5     107\n",
      "6     118\n",
      "7      93\n",
      "8     121\n",
      "9      98\n",
      "10    108\n",
      "11     99\n",
      "12    114\n",
      "13     90\n",
      "14    129\n",
      "15     87\n",
      "16    102\n",
      "17    120\n",
      "18    110\n",
      "19    166\n",
      "20    109\n",
      "21    109\n",
      "22     70\n",
      "23    132\n",
      "24     57\n",
      "25    140\n",
      "26    137\n",
      "27     98\n",
      "28    119\n",
      "29    130\n",
      "30    191\n",
      "31    143\n",
      "32    132\n",
      "33    159\n",
      "34    184\n",
      "35    101\n",
      "36    119\n",
      "37     62\n",
      "38     98\n",
      "39      8\n",
      "Name: Value, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create the first pipeline configuration\n",
    "baseline_pipeline_cfg = Config.configure_pipeline(id=\"baseline\",\n",
    "                                                  task_configs=[clean_data_task_cfg, predict_baseline_task_cfg])\n",
    "\n",
    "import taipy as tp\n",
    "\n",
    "# Create the pipeline\n",
    "baseline_pipeline = tp.create_pipeline(baseline_pipeline_cfg)\n",
    "# Submit the pipeline (Execution)\n",
    "tp.submit(baseline_pipeline)\n",
    "\n",
    "# Read output data from the pipeline\n",
    "baseline_predictions = baseline_pipeline.predictions.read()\n",
    "print(\"Predictions of baseline algorithm\\n\", baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f1abf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the \"predictions\" dataset\n",
    "predictions_dataset = pd.DataFrame({\"Date\":[dt.datetime(2021, 6, 1)], \"Historical values\":[np.NaN], \"Predicted values\":[np.NaN]})\n",
    "\n",
    "# Add a button and a chart for our predictions\n",
    "pipeline_page = page + \"\"\"\n",
    "Press <|predict|button|on_action=predict|> to predict with default parameters (30 predictions) and June 1st as day.\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a349baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(state):\n",
    "    print(\"'Predict' button clicked\")\n",
    "    pipeline = create_and_submit_pipeline()\n",
    "    update_predictions_dataset(state, pipeline)\n",
    "\n",
    "\n",
    "def create_and_submit_pipeline():\n",
    "    print(\"Execution of pipeline...\")\n",
    "    # Create the pipeline from the pipeline config\n",
    "    pipeline = tp.create_pipeline(baseline_pipeline_cfg)\n",
    "    # Submit the pipeline (Execution)\n",
    "    tp.submit(pipeline)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e049e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_dataset(pipeline):\n",
    "    print(\"Creating predictions dataset...\")\n",
    "    # Read data from the pipeline\n",
    "    predictions = pipeline.predictions.read()\n",
    "    day = pipeline.day.read()\n",
    "    n_predictions = pipeline.n_predictions.read()\n",
    "    cleaned_data = pipeline.cleaned_dataset.read()\n",
    "\n",
    "    # Set arbitrarily the time window for the chart as 5 times the number of predictions\n",
    "    window = 5 * n_predictions\n",
    "\n",
    "    # Create the historical dataset that will be displayed\n",
    "    new_length = len(cleaned_data[cleaned_data[\"Date\"] < day]) + n_predictions\n",
    "    temp_df = cleaned_data[:new_length]\n",
    "    temp_df = temp_df[-window:].reset_index(drop=True)\n",
    "\n",
    "    # Create the series that will be used in the concat\n",
    "    historical_values = pd.Series(temp_df[\"Value\"], name=\"Historical values\")\n",
    "    predicted_values = pd.Series([np.NaN]*len(temp_df), name=\"Predicted values\")\n",
    "    predicted_values[-len(predictions):] = predictions\n",
    "\n",
    "    # Create the predictions dataset\n",
    "    # Columns : [Date, Historical values, Predicted values]\n",
    "    return pd.concat([temp_df[\"Date\"], historical_values, predicted_values], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "459257b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_predictions_dataset(state, pipeline):\n",
    "    print(\"Updating predictions dataset...\")\n",
    "    state.predictions_dataset = create_predictions_dataset(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdff110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "Gui(page=pipeline_page).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a0cf272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of clarity, we have used an AutoRegressive model rather than a pure ML model such as:\n",
    "# Random Forest, Linear Regression, LSTM, etc   \n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# This is the function that will be used by the task\n",
    "def predict_ml(cleaned_dataset: pd.DataFrame, n_predictions: int, day: dt.datetime, max_capacity: int):\n",
    "    print(\"     Predicting with ML\")\n",
    "    # Select the train data\n",
    "    train_dataset = cleaned_dataset[cleaned_dataset[\"Date\"] < day]\n",
    "\n",
    "    # Fit the AutoRegressive model\n",
    "    model = AutoReg(train_dataset[\"Value\"], lags=7).fit()\n",
    "\n",
    "    # Get the n_predictions forecasts\n",
    "    predictions = model.forecast(n_predictions).reset_index(drop=True)\n",
    "    predictions = predictions.apply(lambda x: min(x, max_capacity))\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25e15ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the task configuration of the predict_ml function.\n",
    "## We use the same input and ouput as the previous predict_baseline task but we change the funtion\n",
    "predict_ml_task_cfg = Config.configure_task(id=\"predict_ml\",\n",
    "                                            function=predict_ml,\n",
    "                                            input=[cleaned_dataset_cfg, n_predictions_cfg, day_cfg, max_capacity_cfg],\n",
    "                                            output=predictions_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07b77b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new ml pipeline that will clean and predict with the ml model\n",
    "ml_pipeline_cfg = Config.configure_pipeline(id=\"ml\", task_configs=[clean_data_task_cfg, predict_ml_task_cfg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "963394e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the list of pipelines names\n",
    "# It will be used in a selector of pipelines\n",
    "pipeline_selector = [\"baseline\", \"ml\"]\n",
    "selected_pipeline = pipeline_selector[0]\n",
    "scenario_page = page + \"\"\"\n",
    "Select the pipeline\n",
    "<|{selected_pipeline}|selector|lov={pipeline_selector}|> <|Update chart|button|on_action=update_chart|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59c515a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our scenario which is our business problem.\n",
    "scenario_cfg = Config.configure_scenario(id=\"scenario\", pipeline_configs=[baseline_pipeline_cfg, ml_pipeline_cfg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c77f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario():\n",
    "    print(\"Creating scenario...\")\n",
    "    scenario = tp.create_scenario(scenario_cfg)\n",
    "    scenario = submit_scenario(scenario)\n",
    "    return scenario\n",
    "\n",
    "def submit_scenario(scenario):\n",
    "    print(\"Submitting scenario...\")\n",
    "    tp.submit(scenario)\n",
    "    return scenario\n",
    "\n",
    "def update_chart(state):\n",
    "    print(\"'Update chart' button clicked\")\n",
    "    # Select the right pipeline\n",
    "    pipeline = scenario.pipelines[state.selected_pipeline]\n",
    "\n",
    "    # Update the chart based on this pipeline\n",
    "    # It is the same function as created before in step_5\n",
    "    update_predictions_dataset(state, pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6fc856b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all entities\n",
    "Config.configure_global_app(clean_entities_enabled=True)\n",
    "tp.clean_all_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2f1cdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scenario...\n",
      "Submitting scenario...\n",
      "     Cleaning data\n",
      "[2023-01-31 15:38:39,102][Taipy][INFO] job JOB_clean_data_6d16c7f6-4e2e-4c83-aa02-3e93a13159cd is completed.\n",
      "     Predicting baseline\n",
      "[2023-01-31 15:38:39,120][Taipy][INFO] job JOB_predict_baseline_53b18120-41e4-49af-b242-0e6d58bd06fc is completed.\n",
      "[2023-01-31 15:38:39,144][Taipy][INFO] job JOB_clean_data_4b300d03-0996-47a1-b6cd-20837680693b is skipped.\n",
      "     Predicting with ML\n",
      "[2023-01-31 15:38:39,253][Taipy][INFO] job JOB_predict_ml_a81861c0-d072-46ad-a99d-68e124f29970 is completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/deterministic.py:1451: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(self._index, pd.Int64Index):\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/ar_model.py:248: FutureWarning: The parameter names will change after 0.12 is released. Set old_names to False to use the new names now. Set old_names to True to use the old names. \n",
      "  warnings.warn(\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/deterministic.py:141: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(index, pd.Int64Index) and np.all(np.diff(index) == 1):\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/deterministic.py:143: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  return pd.Int64Index(idx_arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# Creation of our first scenario\n",
    "scenario = create_scenario()\n",
    "Gui(page=scenario_page).run(dark_mode=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93d17a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial variables\n",
    "## Initial variables for the scenario   \n",
    "day = dt.datetime(2021, 7, 26)\n",
    "n_predictions = 40\n",
    "max_capacity = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81766cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_scenario_manager = page + \"\"\"\n",
    "# Change your scenario\n",
    "\n",
    "**Prediction date**\\n\\n <|{day}|date|not with_time|>\n",
    "\n",
    "**Max capacity**\\n\\n <|{max_capacity}|number|>\n",
    "\n",
    "**Number of predictions**\\n\\n<|{n_predictions}|number|>\n",
    "\n",
    "<|Save changes|button|on_action={submit_scenario}|>\n",
    "\n",
    "Select the pipeline\n",
    "<|{selected_pipeline}|selector|lov={pipeline_selector}|> <|Update chart|button|on_action={update_chart}|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "094355ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario():\n",
    "    global selected_scenario\n",
    "\n",
    "    print(\"Creating scenario...\")\n",
    "    scenario = tp.create_scenario(scenario_cfg)\n",
    "\n",
    "    selected_scenario = scenario.id\n",
    "\n",
    "    tp.submit(scenario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "514b6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_scenario(state):\n",
    "    print(\"Submitting scenario...\")\n",
    "    # Get the selected scenario: in this current step a single scenario is created then modified here.\n",
    "    scenario = tp.get(selected_scenario)\n",
    "\n",
    "    # Conversion to the right format\n",
    "    state_day = dt.datetime(state.day.year, state.day.month, state.day.day)\n",
    "\n",
    "    # Change the default parameters by writing in the datanodes\n",
    "    scenario.day.write(state_day)\n",
    "    scenario.n_predictions.write(int(state.n_predictions))\n",
    "    scenario.max_capacity.write(int(state.max_capacity))\n",
    "\n",
    "    # Execute the pipelines/code\n",
    "    tp.submit(scenario)\n",
    "\n",
    "    # Update the chart when we change the scenario\n",
    "    update_chart(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90bbf9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scenario...\n",
      "[2023-01-31 15:39:27,078][Taipy][INFO] job JOB_clean_data_2be10bca-c006-48c8-bc66-244049c9b9fb is skipped.\n",
      "     Predicting baseline\n",
      "[2023-01-31 15:39:27,102][Taipy][INFO] job JOB_predict_baseline_467c8a41-723e-40cf-ada2-c561d61aab63 is completed.\n",
      "[2023-01-31 15:39:27,126][Taipy][INFO] job JOB_clean_data_3f647273-fcff-4b61-b797-1cfc55ffc749 is skipped.\n",
      "     Predicting with ML\n",
      "[2023-01-31 15:39:27,153][Taipy][INFO] job JOB_predict_ml_06d02462-67f3-4ef8-b2a5-4aad5c7d4b0e is completed.\n",
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/deterministic.py:1451: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(self._index, pd.Int64Index):\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/ar_model.py:248: FutureWarning: The parameter names will change after 0.12 is released. Set old_names to False to use the new names now. Set old_names to True to use the old names. \n",
      "  warnings.warn(\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/deterministic.py:141: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(index, pd.Int64Index) and np.all(np.diff(index) == 1):\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/deterministic.py:143: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  return pd.Int64Index(idx_arr)\n"
     ]
    }
   ],
   "source": [
    "def update_chart(state):\n",
    "    # Select the right scenario and pipeline\n",
    "    scenario = tp.get(selected_scenario)\n",
    "    pipeline = scenario.pipelines[state.selected_pipeline]\n",
    "    # Update the chart based on this pipeline\n",
    "    update_predictions_dataset(state, pipeline)\n",
    "\n",
    "\n",
    "global selected_scenario\n",
    "# Creation of a single scenario\n",
    "create_scenario()\n",
    "Gui(page=page_scenario_manager).run(dark_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42b9653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the scenarios already created\n",
    "all_scenarios = tp.get_scenarios()\n",
    "\n",
    "# Delete the scenarios that don't have a name attribute\n",
    "# All the scenarios of the previous steps do not have an associated name so they will be deleted,\n",
    "# this will not be the case for those created by this step\n",
    "[tp.delete(scenario.id) for scenario in all_scenarios if scenario.name is None]\n",
    "\n",
    "# Initial variable for the scenario selector\n",
    "# The list of possible values (lov) for the scenario selector is a list of tuples (scenario_id, scenario_name),\n",
    "# but the selected_scenario is just used to retrieve the scenario id and what gets displayed is the name of the scenario.\n",
    "scenario_selector = [(scenario.id, scenario.name) for scenario in tp.get_scenarios()]\n",
    "selected_scenario = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c616d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_manager_page = page + \"\"\"\n",
    "# Create your scenario\n",
    "\n",
    "**Prediction date**\\n\\n <|{day}|date|not with_time|>\n",
    "\n",
    "**Max capacity**\\n\\n <|{max_capacity}|number|>\n",
    "\n",
    "**Number of predictions**\\n\\n<|{n_predictions}|number|>\n",
    "\n",
    "<|Create new scenario|button|on_action=create_scenario|>\n",
    "\n",
    "## Scenario \n",
    "<|{selected_scenario}|selector|lov={scenario_selector}|dropdown|>\n",
    "\n",
    "## Display the pipeline\n",
    "<|{selected_pipeline}|selector|lov={pipeline_selector}|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_name_for_scenario(state)->str:\n",
    "    name = f\"Scenario ({state.day.strftime('%A, %d %b')}; {state.max_capacity}; {state.n_predictions})\"\n",
    "\n",
    "    # Change the name if it is the same as some scenarios\n",
    "    if name in [s[1] for s in state.scenario_selector]:\n",
    "        name += f\" ({len(state.scenario_selector)})\"\n",
    "    return name\n",
    "\n",
    "\n",
    "def update_chart(state):\n",
    "    # Now, the selected_scenario comes from the state, it is interactive\n",
    "    scenario = tp.get(state.selected_scenario[0])\n",
    "    pipeline = scenario.pipelines[state.selected_pipeline]\n",
    "    update_predictions_dataset(state, pipeline)\n",
    "\n",
    "\n",
    "# Change the create_scenario function in order to change the default parameters\n",
    "# and allow the creation of multiple scenarios\n",
    "def create_scenario(state):\n",
    "    print(\"Execution of scenario...\")\n",
    "    # Extra information for the scenario\n",
    "    creation_date = state.day\n",
    "    name = create_name_for_scenario(state)\n",
    "    # Create a scenario\n",
    "    scenario = tp.create_scenario(scenario_cfg,creation_date=creation_date, name=name)\n",
    "\n",
    "    state.selected_scenario = (scenario.id, name)\n",
    "    # Submit the scenario that is currently selected\n",
    "    submit_scenario(state)\n",
    "\n",
    "\n",
    "def submit_scenario(state):\n",
    "    print(\"Submitting scenario...\")\n",
    "    # Get the currently selected scenario\n",
    "    scenario = tp.get(state.selected_scenario[0])\n",
    "\n",
    "    # Conversion to the right format (change?)\n",
    "    day = dt.datetime(state.day.year, state.day.month, state.day.day) \n",
    "\n",
    "    # Change the default parameters by writing in the Data Nodes\n",
    "    scenario.day.write(day)\n",
    "    scenario.n_predictions.write(int(state.n_predictions))\n",
    "    scenario.max_capacity.write(int(state.max_capacity))\n",
    "    scenario.creation_date = state.day\n",
    "\n",
    "\n",
    "    # Execute the scenario\n",
    "    tp.submit(scenario)\n",
    "\n",
    "    # Update the scenario selector and the scenario that is currently selected\n",
    "    update_scenario_selector(state, scenario) # change list to scenario\n",
    "\n",
    "    # Update the chart directly\n",
    "    update_chart(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f175bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "from taipy import Gui\n",
    "\n",
    "def on_menu():\n",
    "    print('Menu function called')\n",
    "\n",
    "Gui(page=\"<|menu|label=Menu|lov={['Data Visualization', 'Scenario Manager']}|on_action=on_menu|>\").run()\n",
    "\n",
    "<|part|render={bool_variable}|\n",
    "Text\n",
    "Or visual elements...\n",
    "|>\n",
    "\n",
    "<|layout|columns=1 1 1|\n",
    "Button in first column <|Press|button|>\n",
    "\n",
    "Second column\n",
    "\n",
    "Third column\n",
    "|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16f07fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first page is the original page\n",
    "# (with the slider and the chart that displays a week of the historical data)\n",
    "page_data_visualization = page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a362a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second page: create scenarios and display results\n",
    "page_scenario_manager = \"\"\"\n",
    "# Create your scenario\n",
    "\n",
    "<|layout|columns=1 1 1 1|\n",
    "<|\n",
    "**Prediction date**\\n\\n <|{day}|date|not with_time|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "**Max capacity**\\n\\n <|{max_capacity}|number|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "**Number of predictions**\\n\\n<|{n_predictions}|number|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "<br/>\\n <|Create new scenario|button|on_action=create_scenario|>\n",
    "|>\n",
    "|>\n",
    "\n",
    "<|part|render={len(scenario_selector) > 0}|\n",
    "<|layout|columns=1 1|\n",
    "<|\n",
    "## Scenario \\n <|{selected_scenario}|selector|lov={scenario_selector}|dropdown|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "## Display the pipeline \\n <|{selected_pipeline}|selector|lov={pipeline_selector}|dropdown|>\n",
    "|>\n",
    "|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "|>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77296f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# Create a menu with our pages\n",
    "multi_pages = \"\"\"\n",
    "<|menu|label=Menu|lov={[\"Data Visualization\", \"Scenario Manager\"]}|on_action=on_menu|>\n",
    "\n",
    "<|part|render={page==\"Data Visualization\"}|\"\"\" + page_data_visualization + \"\"\"|>\n",
    "<|part|render={page==\"Scenario Manager\"}|\"\"\" + page_scenario_manager + \"\"\"|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The initial page is the \"Data Visualization\" page\n",
    "page = \"Data Visualization\"\n",
    "def on_menu(state, var_name: str, fct: str, var_value: list):\n",
    "    # Change the value of the state.page variable in order to render the correct page\n",
    "    state.page = var_value[\"args\"][0]\n",
    "\n",
    "\n",
    "Gui(page=multi_pages).run(dark_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3199ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taipy import Config, Frequency\n",
    "\n",
    "# Create scenarios each week and compare them\n",
    "scenario_daily_cfg = Config.configure_scenario(id=\"scenario\",\n",
    "                                           pipeline_configs=[baseline_pipeline_cfg, ml_pipeline_cfg],\n",
    "                                           frequency=Frequency.DAILY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e70ea6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all entities\n",
    "Config.configure_global_app(clean_entities_enabled=True)\n",
    "tp.clean_all_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "187ade98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the inital scenario selector to see which scenarios are primary\n",
    "scenario_selector = [(scenario.id, (\"*\" if scenario.is_primary else \"\") + scenario.name) for scenario in tp.get_scenarios()]\n",
    "\n",
    "# Redefine update_scenario_selector to add \"*\" in the display name when the scnario is primary\n",
    "def update_scenario_selector(state, scenario):\n",
    "    print(\"Updating scenario selector...\")\n",
    "    # Create the scenario name for the scenario selector\n",
    "    # This name changes dependind whether the scenario is primary or not\n",
    "    scenario_name = (\"*\" if scenario.is_primary else \"\") + scenario.name\n",
    "    print(scenario_name)\n",
    "    # Update the scenario selector\n",
    "    state.scenario_selector += [(scenario.id, scenario_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff22b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the create_scenario function to create a scenario with the selected frequency\n",
    "def create_scenario(state):\n",
    "    print(\"Execution of scenario...\")\n",
    "    # Extra information for scenario\n",
    "    creation_date = state.day\n",
    "    name = create_name_for_scenario(state)\n",
    "\n",
    "    # Create a scenario with the week cycle\n",
    "    scenario = tp.create_scenario(scenario_daily_cfg, creation_date=creation_date, name=name)\n",
    "\n",
    "    state.selected_scenario = (scenario.id, name)\n",
    "\n",
    "    # Change the scenario that is currently selected\n",
    "    submit_scenario(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "182202b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenario_is_primary = None\n",
    "\n",
    "def make_primary(state):\n",
    "    print(\"Making the current scenario primary...\")\n",
    "    scenario = tp.get(state.selected_scenario[0])\n",
    "    # Take the current scenario primary\n",
    "    tp.set_primary(scenario)\n",
    "\n",
    "    # Update the scenario selector accordingly\n",
    "    state.scenario_selector = [(scenario.id, (\"*\" if scenario.is_primary else \"\") + scenario.name)\n",
    "                               for scenario in tp.get_scenarios()]\n",
    "    state.selected_scenario_is_primary = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0efd52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taipy.gui import notify\n",
    "\n",
    "def remove_scenario_from_selector(state, scenario: list):\n",
    "    # Take all the scenarios in the selector that doesn't have the scenario.id\n",
    "    state.scenario_selector = [(s[0], s[1]) for s in state.scenario_selector if s[0] != scenario.id]\n",
    "    state.selected_scenario = state.scenario_selector[-1]\n",
    "\n",
    "def delete_scenario(state):\n",
    "    scenario = tp.get(state.selected_scenario[0])\n",
    "\n",
    "    if scenario.is_primary:\n",
    "        # Notify the user that primary scenarios can not be deleted\n",
    "        notify(state, \"info\", \"Cannot delete the primary scenario\")\n",
    "    else:\n",
    "        # Delete the scenario and the related objects (datanodes, tasks, jobs,...)\n",
    "        tp.delete(scenario.id)\n",
    "\n",
    "        # Update the scenario selector accordingly\n",
    "        remove_scenario_from_selector(state,scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d42b165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a \"Delete scenario\" and a \"Make primary\" buttons\n",
    "page_scenario_manager = \"\"\"\n",
    "# Create your scenario:\n",
    "\n",
    "<|layout|columns=1 1 1 1|\n",
    "<|\n",
    "**Prediction date**\\n\\n <|{day}|date|not with_time|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "**Max capacity**\\n\\n <|{max_capacity}|number|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "**Number of predictions**\\n\\n<|{n_predictions}|number|>\n",
    "|>\n",
    "\n",
    "<|\n",
    "<br/>\n",
    "<br/>\n",
    "<|Create new scenario|button|on_action=create_scenario|>\n",
    "|>\n",
    "|>\n",
    "\n",
    "\n",
    "<|part|render={len(scenario_selector) > 0}|\n",
    "<|layout|columns=1 1|\n",
    "\n",
    "<|layout|columns=1 1|\n",
    "<|\n",
    "## Scenario \\n <|{selected_scenario}|selector|lov={scenario_selector}|dropdown|>\n",
    "|>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<|Delete scenario|button|on_action=delete_scenario|active={len(scenario_selector)>0}|>\n",
    "<|Make primary|button|on_action=make_primary|active={not(selected_scenario_is_primary) and len(scenario_selector)>0}|>\n",
    "|>\n",
    "\n",
    "\n",
    "<|\n",
    "## Display the pipeline \\n <|{selected_pipeline}|selector|lov={pipeline_selector}|dropdown|>\n",
    "|>\n",
    "|>\n",
    "\n",
    "<|{predictions_dataset}|chart|x=Date|y[1]=Historical values|type[1]=bar|y[2]=Predicted values|type[2]=scatter|height=80%|width=100%|>\n",
    "|>\n",
    "\"\"\"\n",
    "\n",
    "# Redefine the multi_pages\n",
    "multi_pages = \"\"\"\n",
    "<|menu|label=Menu|lov={[\"Data Visualization\", \"Scenario Manager\"]}|on_action=on_menu|>\n",
    "\n",
    "<|part|render={page==\"Data Visualization\"}|\"\"\" + page_data_visualization + \"\"\"|>\n",
    "<|part|render={page==\"Scenario Manager\"}|\"\"\" + page_scenario_manager + \"\"\"|>\n",
    "\"\"\"\n",
    "\n",
    "def on_change(state, var_name: str, var_value):\n",
    "    if var_name == \"n_week\":\n",
    "        # Update the dataset when the slider is moved\n",
    "        state.dataset_week = dataset[dataset[\"Date\"].dt.isocalendar().week == var_value]\n",
    "\n",
    "    elif var_name == \"selected_pipeline\" or var_name == \"selected_scenario\":\n",
    "        # Update selected_scenario_is_primary indicating if the current scenario is primary or not\n",
    "        state.selected_scenario_is_primary = tp.get(state.selected_scenario[0]).is_primary\n",
    "\n",
    "        # Check if we can read the Data Node to update the chart\n",
    "        if tp.get(state.selected_scenario[0]).predictions.read() is not None:\n",
    "            update_chart(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a455c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gui server has been stopped.\n",
      " * Server starting on http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "Gui(page=multi_pages).run(dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93a342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
